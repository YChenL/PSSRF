{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2056a212-876d-4baa-82a0-c13c030f9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "sys.path.append(os.path.dirname(os.path.dirname(sys.path[0])))\n",
    "from utilities import *\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e77215c-d19d-4d13-87b6-77f916e11642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmask(audio_model, train_loader, test_loader, args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Now running on : ' + str(device))\n",
    "\n",
    "    # initialize all of the statistics we want to keep track of\n",
    "    batch_time = AverageMeter()\n",
    "    per_sample_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    per_sample_data_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "    per_sample_dnn_time = AverageMeter()\n",
    "    train_acc_meter = AverageMeter()\n",
    "    train_nce_meter = AverageMeter()\n",
    "    progress = []\n",
    "    best_epoch, best_acc = 0, -np.inf\n",
    "    global_step, epoch = 0, 0\n",
    "    start_time = time.time()\n",
    "    exp_dir = args.exp_dir\n",
    "\n",
    "    def _save_progress():\n",
    "        progress.append([epoch, global_step, best_epoch, time.time() - start_time])\n",
    "        with open(\"%s/progress.pkl\" % exp_dir, \"wb\") as f:\n",
    "            pickle.dump(progress, f)\n",
    "\n",
    "    if not isinstance(audio_model, nn.DataParallel):\n",
    "        audio_model = nn.DataParallel(audio_model)\n",
    "\n",
    "    audio_model = audio_model.to(device)\n",
    "    # Set up the optimizer\n",
    "    audio_trainables = [p for p in audio_model.parameters() if p.requires_grad]\n",
    "    print('Total parameter number is : {:.9f} million'.format(sum(p.numel() for p in audio_model.parameters()) / 1e6))\n",
    "    print('Total trainable parameter number is : {:.9f} million'.format(sum(p.numel() for p in audio_trainables) / 1e6))\n",
    "    trainables = audio_trainables\n",
    "    optimizer = torch.optim.Adam(trainables, args.lr, weight_decay=5e-7, betas=(0.95, 0.999))\n",
    "\n",
    "    # LR scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=args.lr_patience, verbose=True)\n",
    "    epoch += 1\n",
    "\n",
    "    print(\"current #steps=%s, #epochs=%s\" % (global_step, epoch))\n",
    "    print(\"start training...\")\n",
    "\n",
    "    result = []\n",
    "    audio_model.train()\n",
    "\n",
    "    # training until break\n",
    "    while epoch < args.n_epochs + 1:\n",
    "        begin_time = time.time()\n",
    "        end_time = time.time()\n",
    "        audio_model.train()\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        # save from-scratch models before the first epoch\n",
    "        torch.save(audio_model.state_dict(), \"%s/models/audio_model.%d.pth\" % (exp_dir, global_step+1))\n",
    "\n",
    "        for i, (audio_input, _) in enumerate(train_loader):\n",
    "            # measure data loading time\n",
    "            B = audio_input.size(0)\n",
    "            audio_input = audio_input.to(device, non_blocking=True)\n",
    "\n",
    "            data_time.update(time.time() - end_time)\n",
    "            per_sample_data_time.update((time.time() - end_time) / audio_input.shape[0])\n",
    "            dnn_start_time = time.time()\n",
    "\n",
    "            # first several steps for warm-up\n",
    "            if global_step <= 1000 and global_step % 50 == 0:\n",
    "                warm_lr = (global_step / 1000) * args.lr\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = warm_lr\n",
    "                print('warm-up learning rate is {:f}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "            # use cluster masking only when masking patches, not frames\n",
    "            cluster = (args.num_mel_bins != args.fshape)\n",
    "            # if pretrain with discriminative objective\n",
    "            if args.task == 'pretrain_mpc':\n",
    "                acc, loss = audio_model(audio_input, args.task, mask_patch=args.mask_patch, cluster=cluster)\n",
    "                # this is for multi-gpu support, in our code, loss is calculated in the model\n",
    "                # pytorch concatenates the output of each gpu, we thus get mean of the losses of each gpu\n",
    "                acc, loss = acc.mean(), loss.mean()\n",
    "            # if pretrain with generative objective\n",
    "            elif args.task == 'pretrain_mpg':\n",
    "                loss = audio_model(audio_input, args.task, mask_patch=args.mask_patch, cluster=cluster)\n",
    "                loss = loss.mean()\n",
    "                # dirty code to make the code report mse loss for generative objective\n",
    "                acc = loss\n",
    "            # if pretrain with joint discriminative and generative objective\n",
    "            elif args.task == 'pretrain_joint':\n",
    "                acc, loss1 = audio_model(audio_input, 'pretrain_mpc', mask_patch=args.mask_patch, cluster=cluster)\n",
    "                acc, loss1 = acc.mean(), loss1.mean()\n",
    "                loss2 = audio_model(audio_input, 'pretrain_mpg', mask_patch=args.mask_patch, cluster=cluster)\n",
    "                loss2 = loss2.mean()\n",
    "                loss = loss1 + 10 * loss2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # record loss\n",
    "            train_acc_meter.update(acc.detach().cpu().item())\n",
    "            train_nce_meter.update(loss.detach().cpu().item())\n",
    "            loss_meter.update(loss.item(), B)\n",
    "            batch_time.update(time.time() - end_time)\n",
    "            per_sample_time.update((time.time() - end_time)/audio_input.shape[0])\n",
    "            per_sample_dnn_time.update((time.time() - dnn_start_time)/audio_input.shape[0])\n",
    "\n",
    "            print_step = global_step % args.n_print_steps == 0\n",
    "            early_print_step = epoch == 0 and global_step % (args.n_print_steps/10) == 0\n",
    "            print_step = print_step or early_print_step\n",
    "\n",
    "            if print_step and global_step != 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Per Sample Total Time {per_sample_time.avg:.5f}\\t'\n",
    "                  'Per Sample Data Time {per_sample_data_time.avg:.5f}\\t'\n",
    "                  'Per Sample DNN Time {per_sample_dnn_time.avg:.5f}\\t'\n",
    "                  'Train Loss {loss_meter.val:.4f}\\t'.format(\n",
    "                   epoch, i, len(train_loader), per_sample_time=per_sample_time, per_sample_data_time=per_sample_data_time,\n",
    "                      per_sample_dnn_time=per_sample_dnn_time, loss_meter=loss_meter), flush=True)\n",
    "                if np.isnan(loss_meter.avg):\n",
    "                    print(\"training diverged...\")\n",
    "                    return\n",
    "\n",
    "            end_time = time.time()\n",
    "            global_step += 1\n",
    "\n",
    "            # pretraining data is usually very large, save model every epoch is too sparse.\n",
    "            # save the model every args.epoch_iter steps.\n",
    "            epoch_iteration = args.epoch_iter\n",
    "            if global_step % epoch_iteration == 0:\n",
    "                print('---------------- step '+ str(global_step) +' evaluation ----------------')\n",
    "                equ_epoch = int(global_step/epoch_iteration) + 1\n",
    "                acc_eval, nce_eval = validatemask(audio_model, test_loader, args, equ_epoch)\n",
    "\n",
    "                print(\"masked acc train: {:.6f}\".format(acc))\n",
    "                print(\"nce loss train: {:.6f}\".format(loss))\n",
    "                print(\"masked acc eval: {:.6f}\".format(acc_eval))\n",
    "                print(\"nce loss eval: {:.6f}\".format(nce_eval))\n",
    "                result.append([train_acc_meter.avg, train_nce_meter.avg, acc_eval, nce_eval, optimizer.param_groups[0]['lr']])\n",
    "                np.savetxt(exp_dir + '/result.csv', result, delimiter=',')\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    torch.save(audio_model.state_dict(), \"%s/models/best_audio_model.pth\" % (exp_dir))\n",
    "\n",
    "                torch.save(audio_model.state_dict(), \"%s/models/audio_model.%d.pth\" % (exp_dir, equ_epoch))\n",
    "                if len(train_loader.dataset) > 2e5:\n",
    "                    torch.save(optimizer.state_dict(), \"%s/models/optim_state.pth\" % (exp_dir))\n",
    "\n",
    "                # if the task is generation, stop after eval mse loss stop improve\n",
    "                if args.task == 'pretrain_mpg':\n",
    "                    # acc_eval is in fact the mse loss, it is dirty code\n",
    "                    scheduler.step(-acc_eval)\n",
    "                else:\n",
    "                    scheduler.step(acc_eval)\n",
    "\n",
    "                print('# {:d}, step {:d}-{:d}, lr: {:e}'.format(equ_epoch, global_step-epoch_iteration, global_step, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "                _save_progress()\n",
    "\n",
    "                finish_time = time.time()\n",
    "                print('# {:d}, step {:d}-{:d}, training time: {:.3f}'.format(equ_epoch, global_step-epoch_iteration, global_step, finish_time-begin_time))\n",
    "                begin_time = time.time()\n",
    "\n",
    "                train_acc_meter.reset()\n",
    "                train_nce_meter.reset()\n",
    "                batch_time.reset()\n",
    "                per_sample_time.reset()\n",
    "                data_time.reset()\n",
    "                per_sample_data_time.reset()\n",
    "                loss_meter.reset()\n",
    "                per_sample_dnn_time.reset()\n",
    "\n",
    "                # change the models back to train mode\n",
    "                audio_model.train()\n",
    "                print('---------------- evaluation finished ----------------')\n",
    "        epoch += 1\n",
    "\n",
    "def validatemask(audio_model, val_loader, args, epoch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if not isinstance(audio_model, nn.DataParallel):\n",
    "        audio_model = nn.DataParallel(audio_model)\n",
    "    audio_model = audio_model.to(device)\n",
    "    # switch to evaluate mode\n",
    "    audio_model.eval()\n",
    "\n",
    "    A_acc = []\n",
    "    A_nce = []\n",
    "    with torch.no_grad():\n",
    "        for i, (audio_input, _) in enumerate(val_loader):\n",
    "            audio_input = audio_input.to(device)\n",
    "\n",
    "            # use cluster masking only when masking patches, not frames\n",
    "            cluster = (args.num_mel_bins != args.fshape)\n",
    "            # always use mask_patch=400 for evaluation, even the training mask patch number differs.\n",
    "            if args.task == 'pretrain_mpc':\n",
    "                acc, nce = audio_model(audio_input, args.task, mask_patch=400, cluster=cluster)\n",
    "                A_acc.append(torch.mean(acc).cpu())\n",
    "                A_nce.append(torch.mean(nce).cpu())\n",
    "            elif args.task == 'pretrain_mpg':\n",
    "                mse = audio_model(audio_input, args.task, mask_patch=400, cluster=cluster)\n",
    "                # this is dirty code to track mse loss, A_acc and A_nce now track mse, not the name suggests\n",
    "                A_acc.append(torch.mean(mse).cpu())\n",
    "                A_nce.append(torch.mean(mse).cpu())\n",
    "            elif args.task == 'pretrain_joint':\n",
    "                acc, _ = audio_model(audio_input, 'pretrain_mpc', mask_patch=400, cluster=cluster)\n",
    "                mse = audio_model(audio_input, 'pretrain_mpg', mask_patch=400, cluster=cluster)\n",
    "\n",
    "                A_acc.append(torch.mean(acc).cpu())\n",
    "                # A_nce then tracks the mse loss\n",
    "                A_nce.append(torch.mean(mse).cpu())\n",
    "\n",
    "        acc = np.mean(A_acc)\n",
    "        nce = np.mean(A_nce)\n",
    "\n",
    "    return acc, nce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea45d3-95e0-470b-a13c-34a8fc003e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
